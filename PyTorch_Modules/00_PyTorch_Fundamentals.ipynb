{"cells":[{"cell_type":"markdown","metadata":{"id":"5MjBNrhFhlZl"},"source":["# **PyTorch Fundementals**\n","\n","Resource notebook: https://www.learnpytorch.io/00_pytorch_fundamentals/\n","\n","Forum for questions: https://github.com/mrdbourke/pytorch-deep-learning/discussions*\n","\n","Extra exercises and curriculum for section 00: https://www.learnpytorch.io/00_pytorch_fundamentals/#exercises"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":318,"status":"ok","timestamp":1684075861921,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"d5sA-4FWhtNe","outputId":"ebcd8b7e-e68e-48ca-ed34-f83e349674d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.0.0+cu118\n"]}],"source":["import torch\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","print(torch.__version__)"]},{"cell_type":"markdown","metadata":{"id":"HbOYfb8dnTUU"},"source":["## **Introduction to Tensors**\n"]},{"cell_type":"markdown","metadata":{"id":"Sz8GEtV5aTB8"},"source":["### **Creating tensors**\n","\n","PyTorch tensors are created using 'torch.Tensor()' = https://pytorch.org/docs/stable/tensors.html"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1684075862699,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"erxMyYYJoUO4","outputId":"8a9e2cea-a322-46bd-99ad-3b4427242f99"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(7)"]},"metadata":{},"execution_count":64}],"source":["# scalar\n","scalar = torch.tensor(7)\n","scalar"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684075862700,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"5_FqvWQeo6tc","outputId":"bbe84c2e-e182-4212-e28e-4050e7e59679"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":65}],"source":["scalar.ndim"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684075862700,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"pnrzM2jxpVz_","outputId":"dac633b5-b33e-4a0e-f0f6-48523be28470"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{},"execution_count":66}],"source":["# Get tensor back as Python int\n","scalar.item()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684075862700,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"1mCsKwOrpgHv","outputId":"8ff8e06c-f617-4610-b2bb-e5ed0cc6c8f0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([7, 7])"]},"metadata":{},"execution_count":67}],"source":["# Vector\n","vector = torch.tensor([7, 7])\n","vector"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684075862700,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"76_ElfoXp2qX","outputId":"c53e784e-6f3c-47ff-f0da-eae13855abde"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":68}],"source":["vector.ndim"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684075862700,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"4RVoLEx4p84K","outputId":"97addd50-c478-4b43-9015-5cfc2d397fef"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2])"]},"metadata":{},"execution_count":69}],"source":["vector.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684075862700,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"KvgDpv8gqD6r","outputId":"5e51bf5c-0fec-4f39-e892-19474b76f244"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 7,  8],\n","        [ 9, 10]])"]},"metadata":{},"execution_count":70}],"source":["# MATRIX\n","MATRIX = torch.tensor([[7, 8],[9, 10]])\n","MATRIX"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684075862700,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"Je_fSCXeqWk0","outputId":"d576315b-42a8-47e8-ff40-d8dd90d0ec21"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":71}],"source":["MATRIX.ndim"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684075862700,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"uJ3Ys6omqapd","outputId":"92c53ee5-d595-454e-b75d-87d4d2b3143b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 9, 10])"]},"metadata":{},"execution_count":72}],"source":["MATRIX[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684075862700,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"fK9tssgkqgTF","outputId":"d8a4c142-40b7-4974-8245-d1565a5ade11"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([7, 8])"]},"metadata":{},"execution_count":73}],"source":["MATRIX[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684075862700,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"Owr6nVgdqi4N","outputId":"1e82ea47-fe7b-4cbd-95da-deb2f99798b2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 2])"]},"metadata":{},"execution_count":74}],"source":["MATRIX.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1684075862700,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"cIKhQjRzq02P","outputId":"6420185f-ad50-45fa-ba1a-1997e6e2761f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1, 2, 3],\n","         [3, 6, 9],\n","         [2, 4, 5]]])"]},"metadata":{},"execution_count":75}],"source":["# TENSOR\n","TENSOR = torch.tensor([[[1, 2, 3],[3, 6, 9],[2, 4, 5]]])\n","TENSOR"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1684075862700,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"4trDcOVArRuh","outputId":"1319a2c1-cdec-4311-b69d-1551a960c205"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":76}],"source":["TENSOR.ndim"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684075862701,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"mS-ByqJxrV1Z","outputId":"05b35083-7392-478c-b780-61aaa47f1ead"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 3, 3])"]},"metadata":{},"execution_count":77}],"source":["TENSOR.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684075862701,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"oXHiGYjBrgUS","outputId":"6e78587f-90e3-407a-f8b4-298e18079a98"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3],\n","        [3, 6, 9],\n","        [2, 4, 5]])"]},"metadata":{},"execution_count":78}],"source":["TENSOR[0]"]},{"cell_type":"markdown","metadata":{"id":"xYyR3avBXa3E"},"source":["### **Random Tensors**\n","\n","Why random tensors? \n","\n","Random tensors are important because the way many neural networks learn is that they start with tensors full of a random numbers and adjust those random numbers to better represent the data. \n","\n","Start with random numbers -> look at data -> update random numbers -> look at data -> udate random numbers\n","\n","Torch random tensors - https://pytorch.org/docs/stable/generated/torch.rand.html\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1684075862701,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"T3A8jhK_XfqE","outputId":"db712f9b-4148-4191-c756-962a62eab0ea"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.4058, 0.7902, 0.1374, 0.5764],\n","         [0.9913, 0.4002, 0.0213, 0.8762],\n","         [0.0300, 0.4699, 0.1480, 0.8039]]])"]},"metadata":{},"execution_count":79}],"source":["# Create a random tensor of size (3, 4)\n","random_tensor = torch.rand(1, 3, 4)\n","random_tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1684075862701,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"HsSKkd3gbSoq","outputId":"fd3ff1ee-4c1d-4d42-9729-4d630bb97c34"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":80}],"source":["random_tensor.ndim"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1684075863070,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"0AkqCbkKbv09","outputId":"e47673d0-03ca-4c91-e12a-452509a66665"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([224, 224, 3]), 3)"]},"metadata":{},"execution_count":81}],"source":["# Create a random tesnor with similar shape to an image tensor\n","random_image_size_tensor = torch.rand(size=(224,224,3)) # height, width, color channels (R,G,B)\n","random_image_size_tensor.shape, random_image_size_tensor.ndim"]},{"cell_type":"markdown","metadata":{"id":"OLJhCuD9c9yL"},"source":["### **Zeros and Ones Tensors**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684075863070,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"BN_T_PB1dB0D","outputId":"a7ea81b8-0314-40fa-f96b-725b9c1c843e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0., 0., 0.],\n","        [0., 0., 0., 0.],\n","        [0., 0., 0., 0.]])"]},"metadata":{},"execution_count":82}],"source":["# Create a tensor of all zeros\n","zeros = torch.zeros(size = (3, 4))\n","zeros"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684075863070,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"jq6LK9LBdQ30","outputId":"0b6a9450-e9bc-4549-c10a-b873eb9998a4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]]])"]},"metadata":{},"execution_count":83}],"source":["zeros*random_tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684075863070,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"1JdDjUhwdc81","outputId":"5bfddc09-1b06-47df-f568-0cb28a70ac81"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1., 1., 1.],\n","        [1., 1., 1., 1.],\n","        [1., 1., 1., 1.]])"]},"metadata":{},"execution_count":84}],"source":["# Create a tensor of all ones\n","ones = torch.ones(size=(3, 4))\n","ones"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684075863070,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"HGk9aacPdmPW","outputId":"59ac9eaf-c16f-4ec8-aa58-538e433a7c07"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.float32"]},"metadata":{},"execution_count":85}],"source":["ones.dtype"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684075863070,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"LiBA4Czddsde","outputId":"02dd2535-fdd8-4c83-c319-00220629732c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.float32"]},"metadata":{},"execution_count":86}],"source":["random_tensor.dtype"]},{"cell_type":"markdown","metadata":{"id":"jkxKKJfheNrx"},"source":["### **Range of Tensors**\n","\n","Torch arange - https://pytorch.org/docs/stable/generated/torch.arange.html"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684075863070,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"6HyBjTw8eTeD","outputId":"12c14ea3-df8e-4477-e51b-d127eacb2a6a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"]},"metadata":{},"execution_count":87}],"source":["# Use torch.range()\n","one_to_ten = torch.arange(start = 1, end = 11, step = 1)\n","one_to_ten"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684075863070,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"GWt-Hak-fVbO","outputId":"8d13456b-789f-4dc1-8cef-187a8c19e177"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"]},"metadata":{},"execution_count":88}],"source":["# Creating tensors like\n","ten_zeros = torch.zeros_like(input=one_to_ten)\n","ten_zeros"]},{"cell_type":"markdown","metadata":{"id":"JlTRjzODgnoO"},"source":["### **Tensor datatypes**\n","\n","***Note*** datatype errors are one of the 3 most common errors in PyTorch and deep learning\n","1. Tensors not right datatype\n","2. Tensors not right shape\n","3. Tensors not on the right device\n","\n","Precision in Computing: https://en.wikipedia.org/wiki/Precision_(computer_science)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684075863070,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"WWj_6oVtg5Ow","outputId":"c864fbbc-4059-4fcb-fdf6-7374835b6bb4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([3., 6., 9.])"]},"metadata":{},"execution_count":89}],"source":["# Float 32 tensor\n","float_32_tensor = torch.tensor([3.0, 6.0, 9.0], dtype = None, # What datatype is the tensor?\n","                                                device = None, # What device is tensor on?\n","                                                requires_grad = False) # Track Gradient?\n","float_32_tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684075863070,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"-iM6wFnZjWnn","outputId":"71b48710-d5b6-482b-dcfd-26897e6d70e9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.float32"]},"metadata":{},"execution_count":90}],"source":["float_32_tensor.dtype"]},{"cell_type":"markdown","metadata":{"id":"h6tqhJVLmMbu"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684075863070,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"qxD-vdhxjaUv","outputId":"a989ec70-e4cf-41ef-a660-8dc0ea4d641b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([3., 6., 9.], dtype=torch.float16)"]},"metadata":{},"execution_count":91}],"source":["float_16_tensor = float_32_tensor.type(torch.float16)\n","float_16_tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684075863071,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"PbpJHzKomSVn","outputId":"55b90fcc-552e-4ca8-855f-71f913c2fbff"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 9., 36., 81.])"]},"metadata":{},"execution_count":92}],"source":["float_16_tensor * float_32_tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684075863071,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"VAeUbUQTmlYY","outputId":"ee5f82a3-d014-4e3a-b62f-b79cea56a830"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([3, 6, 9])"]},"metadata":{},"execution_count":93}],"source":["int_32_tensor = torch.tensor([3, 6, 9], dtype = torch.int64)\n","int_32_tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684075863071,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"yZcZYHc4m2bS","outputId":"e6841208-a9a2-48fd-d55b-bee569d5a7e3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 9., 36., 81.])"]},"metadata":{},"execution_count":94}],"source":["float_32_tensor * int_32_tensor"]},{"cell_type":"markdown","metadata":{"id":"IUh8kzYJnVuU"},"source":["### **Getting Information from Tesnors** (tensor attributes)\n","\n","1. Tensors not right datatype - to get datatype from a tensor, can use 'tensor.dtype'\n","2. Tensors not right shape - to get shape from tensor, can use 'tensor.shape'\n","3. Tensors not on the right device - to get device of tensor, can use 'tensor.device'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684075863071,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"HPiK91uTn05m","outputId":"d366b5ba-057e-4419-ed88-2de74be642d1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.1081, 0.8023, 0.2287, 0.3849],\n","        [0.2831, 0.4520, 0.7622, 0.4431],\n","        [0.3195, 0.7878, 0.5673, 0.5635]])"]},"metadata":{},"execution_count":95}],"source":["# Create a tensor\n","some_tensor = torch.rand(3, 4)\n","some_tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684075863071,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"JmYZkigMn9gT","outputId":"52e5072f-7b80-4d88-84f7-ca12f4901380"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1081, 0.8023, 0.2287, 0.3849],\n","        [0.2831, 0.4520, 0.7622, 0.4431],\n","        [0.3195, 0.7878, 0.5673, 0.5635]])\n","Datatype of tensor: torch.float32\n","Shape of tensor: torch.Size([3, 4])\n","Device of tensor: cpu\n"]}],"source":["# Find out details about tensor\n","print(some_tensor)\n","print(f\"Datatype of tensor: {some_tensor.dtype}\")\n","print(f\"Shape of tensor: {some_tensor.shape}\")\n","print(f\"Device of tensor: {some_tensor.device}\")"]},{"cell_type":"markdown","metadata":{"id":"3V0fS4v1pRaW"},"source":["### **Manipulating Tensors** (tensor operations)\n","\n","Tensor operations include: \n","* Addition\n","* Subtraction\n","* Multiplication (element-wise)\n","* Division\n","* Matrix multiplication\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1684075863071,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"61Obmd81p4oq","outputId":"bfb7dd69-d5a6-49fe-d25f-46e92862be64"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([11, 12, 13])"]},"metadata":{},"execution_count":97}],"source":["# Create a tensor and add 10 to it\n","tensor = torch.tensor([1, 2, 3])\n","tensor + 10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1684075863417,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"KowZimHqpc58","outputId":"cca56ad5-d5e9-463f-99c9-e2f1de1d7af7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([10, 20, 30])"]},"metadata":{},"execution_count":98}],"source":["# Multiply tensor by 10\n","tensor * 10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1684075863417,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"zTPc41HMqIQJ","outputId":"f72d20c1-b2bb-467f-c50b-cb677a40cd09"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3])"]},"metadata":{},"execution_count":99}],"source":["tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1684075863417,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"ZUqpmwO1rzU0","outputId":"008f98df-0b53-41d6-b829-a539a84c7cca"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-9, -8, -7])"]},"metadata":{},"execution_count":100}],"source":["# Subtract 10\n","tensor - 10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1684075863417,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"_gEiWMojr33M","outputId":"e4fc4cf1-6816-41fc-f66b-28557b5941ba"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([10, 20, 30])"]},"metadata":{},"execution_count":101}],"source":["# Try out PyTorch in-built functions\n","torch.mul(tensor, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684075863417,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"bsO_SKyJsAQd","outputId":"066c5901-7cd1-4b2d-c808-c1bec81cbda5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([11, 12, 13])"]},"metadata":{},"execution_count":102}],"source":["torch.add(tensor, 10)"]},{"cell_type":"markdown","metadata":{"id":"53XS8246sIRK"},"source":["### **Matrix Multiplication**\n","\n","Two main ways of performing multiplication\n","\n","1. Elementwise\n","2. Matrix (dot product . )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684075863417,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"IigqKyX0sfLh","outputId":"6762755e-2414-4efa-9fd5-653d0cebac24"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1, 2, 3]) * tensor([1, 2, 3])\n","Equals: tensor([1, 4, 9])\n"]}],"source":["# Elementwise multiplication\n","print(tensor, \"*\", tensor)\n","print(f\"Equals: {tensor * tensor}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684075863417,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"wpesZlVmtD-p","outputId":"686c8bdb-3a1f-4b0d-dff3-8af2b890c0b3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(14)"]},"metadata":{},"execution_count":104}],"source":["# Matrix multiplication\n","torch.matmul(tensor, tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684075863417,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"Ye7zOAjbtMaf","outputId":"f309be68-9ea2-461e-f269-f8b76f75bb53"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(14)\n","CPU times: user 359 µs, sys: 917 µs, total: 1.28 ms\n","Wall time: 1.19 ms\n"]}],"source":["%%time\n","value = 0\n","for i in range(len(tensor)):\n","  value += tensor[i] * tensor[i]\n","print(value)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684075863417,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"K6ldL-ShtmQq","outputId":"60d11a65-e061-4fe8-ebf0-fd6ba4b436d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 851 µs, sys: 0 ns, total: 851 µs\n","Wall time: 790 µs\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor(14)"]},"metadata":{},"execution_count":106}],"source":["%%time\n","torch.matmul(tensor,tensor)"]},{"cell_type":"markdown","metadata":{"id":"llnJSEU28YCE"},"source":["### **Shape Errors** \n","* One of the most common errors in deep learning\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jfSGgPQv8fZK"},"outputs":[],"source":["#   Shapes for Matrix Multiplication\n","# tensor_A = torch.tensor([[1, 2],[3, 4],[5, 6]])\n","\n","# tensor_B = torch.tensor([[7, 8],[9, 10],[11, 12]])\n","\n","# torch.mm(tensor_A, tensor_B) # torch.mm = torch.matmul\n","# Note: this line produces a shape error, commented out so as not to cause runtime issues"]},{"cell_type":"markdown","metadata":{"id":"h6AliqK0-Mxv"},"source":["* To fix shape error, we will manipulate the shape of one tensor using **transpose**.\n","* A **transpose** switches the axes or dimensions of a given tensor \n","\n","See also, torch.transpose: https://pytorch.org/docs/stable/generated/torch.transpose.html"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":165},"executionInfo":{"elapsed":6,"status":"error","timestamp":1684075863418,"user":{"displayName":"Marcus Worrell","userId":"07752237668680934892"},"user_tz":300},"id":"ZYXy4T3Z-yKH","outputId":"387ff360-0b05-4d10-ccb9-95b53e763680"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-108-b89eb5988a7c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtensor_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_B\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'tensor_B' is not defined"]}],"source":["tensor_B, tensor_B.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lPTsemmC-Wvb"},"outputs":[],"source":["tensor_B.T, tensor_B.T.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yfX3pu97-0e5"},"outputs":[],"source":["# Matrix multiplication works when tensor_B is  (using .T)\n","print(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\")\n","print(f\"\\nNew shapes: tensor_A = {tensor_A.shape}, tensor_B.T = {tensor_B.T.shape}\")\n","print(f\"\\nMultiplying: {tensor_A.shape} @ {tensor_B.T.shape} <- inner dimensions must match \")\n","output = torch.mm(tensor_A, tensor_B.T) # transpose of B\n","print(f\"\\nOuput: {output}\\n\")\n","print(f\"\\nOutput Shape: {output.shape}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"2GGEevGzCexF"},"source":["### **Tensor Aggregation**\n","\n","* Min, Max, Mean and Sum\n","\n","**Documentation:** \n","\n","Max: https://pytorch.org/docs/stable/generated/torch.max.html <br>\n","Min: https://pytorch.org/docs/stable/generated/torch.min.html <br>\n","Mean: https://pytorch.org/docs/stable/generated/torch.mean.html <br>\n","Sum: https://pytorch.org/docs/stable/generated/torch.sum.html <br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vF_8szpKCptv"},"outputs":[],"source":["# Create a tenso\n","x = torch.arange(0, 100, 10)\n","x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3qOxObRxEq6p"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JGgx5LahDE_b"},"outputs":[],"source":["# Find the min\n","torch.min(x), x.min()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q1xRkYymDMkI"},"outputs":[],"source":["# Find the max\n","torch.max(x), x.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mfi7nH0MDWSe"},"outputs":[],"source":["# Find the mean\n","torch.mean(x.type(torch.float32)), x.type(torch.float32).mean() # convert to float, otherwise dtype error \n","# note: torch.mean() function requires a tensor of float32 dtype"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BxHSJ6qpDcSk"},"outputs":[],"source":["# Find the sum\n","torch.sum(x), x.sum()"]},{"cell_type":"markdown","metadata":{"id":"AyAWsuBRF3hn"},"source":["### **Positional Min and Max**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"07hMn_26eP_Z"},"outputs":[],"source":["x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_z-afQHveRcl"},"outputs":[],"source":["# .argmin() returns the index position of the tensor's minimum value\n","x.argmin()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VG7CJZlheVOX"},"outputs":[],"source":["x[0] # standard index for position 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DP1S-cpweXuQ"},"outputs":[],"source":["# .argmax() returns index position of the tensor's maximum value\n","x.argmax()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BXJf8qtAfCIl"},"outputs":[],"source":["x[9] # standard index for position 9"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oPzyDOeFfErS"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"2W6WWifPgCyT"},"source":["\n","### **Tensor Manipulation/Tensor Transformation**\n","\n","**Definitions:**\n","* torch.reshape - Returns a tensor with the same data and number of elements as input, but with the specified shape.\n","* torch.Tensor.view - Return a view of an input tensor of certain shapes but keeps the same memory as the original tensor.\n","* torch.stack - Concatenates a sequence of tensors along a new dimension.\n","* torch.vstack - Concatenates a sequence of tensors along a new dimension.\n","* torch.hstack - Stack tensors in sequence horizontally (column wise).\n","* torch.squeeze - Returns a tensor with all specified dimensions of input of size 1 removed.\n","* torch.unsqueeze - Returns a new tensor with a dimension of size one inserted at the specified position.\n","* torch.permute - Returns a view of the original tensor input with its dimensions permuted.\n","\n","**Documentation:** <br>\n","* torch.reshape - https://pytorch.org/docs/stable/generated/torch.reshape.html <br>\n","* torch.Tensor.view - https://pytorch.org/docs/stable/generated/torch.Tensor.view.html <br>\n","* torch.stack - https://pytorch.org/docs/stable/generated/torch.stack.html <br>\n","* torch.vstack - https://pytorch.org/docs/stable/generated/torch.vstack.html <br>\n","* torch.hstack - https://pytorch.org/docs/stable/generated/torch.hstack.html <br>\n","* torch.squeeze - https://pytorch.org/docs/stable/generated/torch.squeeze.html <br>\n","* torch.unsqueeze - https://pytorch.org/docs/stable/generated/torch.unsqueeze.html <br>\n","* torch.permute: https://pytorch.org/docs/stable/generated/torch.permute.html <br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d3t41Dk2gwBg"},"outputs":[],"source":["# Create a tensor\n","import torch\n","x = torch.arange(1., 11.)\n","x, x.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRyfTO0pl308"},"outputs":[],"source":["# Add extra dimension\n","x_reshaped = x.reshape(1, 10)\n","x_reshaped, x_reshaped.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HPjaD4DzmFLZ"},"outputs":[],"source":["# Add extra dimension and make colmumn\n","x_reshaped = x.reshape(10, 1)\n","x_reshaped, x_reshaped.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VYTb7c-_nMJO"},"outputs":[],"source":["# reshape to 2 colums\n","x_reshaped = x.reshape(5, 2)\n","x_reshaped, x_reshaped.shape\n","# note: reshape must match the number of original dimensions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWSUTQX0m8Ll"},"outputs":[],"source":["# Change the view\n","z = x.view(1, 10)\n","z, z.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_gubBSOCoDoQ"},"outputs":[],"source":["# Changing z changes x (because a view of a tensor shares the same memory as the original)\n","z[:, 0] = 5\n","z, x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CUvPxiUloY6w"},"outputs":[],"source":["# Stack tensors vertically\n","x_stacked = torch.stack([x, x, x, x], dim = 1)\n","x_stacked"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o9CiYM-QpA-Y"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HHz26nrsojMG"},"outputs":[],"source":["# Stack tensors vertically\n","x_stacked = torch.stack([x, x, x, x], dim = 0)\n","x_stacked"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FhdycBGCpQOR"},"outputs":[],"source":["# Using .vstack (same as dim = 0)\n","x_vstacked = torch.vstack([x, x, x, x])\n","x_vstacked"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wKJoQ2Q3pdyA"},"outputs":[],"source":["# Using .hstack \n","x_hstacked = torch.hstack([x, x, x, x])\n","x_hstacked"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dR4e-TU2tEfh"},"outputs":[],"source":["# torch.squeeze() - removes all single dimensions from a target tensor\n","import torch\n","y = torch.arange(1., 10.)\n","y, y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uzk1nApJttkh"},"outputs":[],"source":["# Add extra dimension\n","y_reshaped = y.reshape(1, 9)\n","y_reshaped, y_reshaped.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"opa1ZQ09uBHb"},"outputs":[],"source":["# Remove the extra dimension (squeeze)\n","y_squeeze = y_reshaped.squeeze()\n","y_squeeze, y_squeeze.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_nEN2ZPu-wn"},"outputs":[],"source":["# torch.squeeze() print statements\n","print(f\"Previous tensor: {y_reshaped}\")\n","print(f\"Previous shape: {y_reshaped.shape}\")\n","print(f\"\\nNew tensor: {y_squeeze}\")\n","print(f\"New shape: {y_squeeze.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhNI9XqQuXES"},"outputs":[],"source":["# torch.unsqueeze() - Returns a new tensor with a dimension of size one inserted at the specified position.\n","print(f\"Previous tensor: {y_squeeze} \")\n","print(f\"Previous shape: {y_squeeze.shape}\")\n","\n","# add dimension with unsqueeze\n","y_unsqueezed = y_squeeze.unsqueeze(dim = 0)\n","\n","print(f\"\\nNew tensor: {y_unsqueezed}\")\n","print(f\"New shape: {y_unsqueezed.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q0zhxtZZyXGH"},"outputs":[],"source":["# torch.permute - Returns a view of the original tensor input with its dimensions permuted.\n","a_original = torch.rand(size = (224, 224, 3)) #(height, width, color channels)\n","\n","# permute the original to rearrange the axis (or dim) order\n","a_permuted = a_original.permute(2, 0, 1) # shifts axis 0 -> 1, 1 -> 2, 2 -> 0.\n","\n","print(f\"Previous shape: {a_original.shape}\")\n","print(f\"\\nNew shape: {a_permuted.shape}\") #(color channels, height, width)"]},{"cell_type":"markdown","source":["### **Indexing and Slicing**\n","\n","* In PyTorch, indexing works very similarly to how it does in NumPy. It allows you to select or modify portions of your tensor using the values of indices.\n","* Slicing: Similar to Python lists and numpy arrays, you can use slicing to get parts of the tensor.\n","* Integer array indexing: You can index with other tensors.\n","* Boolean mask indexing: You can index with boolean tensors.\n","\n","**Documetation**\n","\n","Indexing: https://pytorch.org/cppdocs/notes/tensor_indexing.html"],"metadata":{"id":"yj21Do6cRlUN"}},{"cell_type":"code","source":["# create a tensor 3D tensor\n","import torch \n","x = torch.arange(1, 10).reshape(1, 3, 3)\n","x, x.shape"],"metadata":{"id":"K-NwXyusRprd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Index on tensor (x)\n","x[0]"],"metadata":{"id":"2rKRRPDSSPjJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Index on middle bracket (dim = 1)\n","x[0][0]"],"metadata":{"id":"aXW6YLEySZEJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Index on innermost bracked (last dimension)\n","x[0][0][0]"],"metadata":{"id":"hxt-urJbShLq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# x[1] is outside the defined tensor range, but how can we index to the 9? \n","x[0][2][2]"],"metadata":{"id":"0g2f6AnnSond"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use \":\" to select all of a target dimension\n","x[:,0], x[:,1], x[:,2]"],"metadata":{"id":"yFdCzE3PTDAs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get all values of 0th and 1st dimensions, but only index 1 of 2nd dimensions\n","x[:, :, 0], x[:, :, 1], x[:, :, 2], x[:, 2, 2]\n","# Note: x[:, 2, 2] is the same as x[0][2][2] except the tensor is returned with brackets around the 9 which means? "],"metadata":{"id":"6DViAkuMT5hN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Basic Indexing\n","# Create a 2D tensor\n","x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n","\n","# Index into the tensor\n","print(x[0, 1]) \n","\n","# In this example, x[0, 1] returns the element at the first row and the second column of the tensor"],"metadata":{"id":"ZQOdv9Y2APkw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In addition to basic indexing, PyTorch supports other advanced indexing techniques:"],"metadata":{"id":"Y6f5Y32mBMnt"}},{"cell_type":"code","source":["# Slicing: Similar to Python lists and numpy arrays, you can use slicing to get parts of the tensor.\n","print(x[:, 1]) \n","\n","\n","# In this example, x[:, 1] returns all the elements in the second column."],"metadata":{"id":"pF2UqHnSA3Rd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Integer array indexing: You can index with other tensors.\n","print(x[[1, 2], [0, 2]])  \n","\n","# In this example, x[[1, 2], [0, 2]] returns the elements at (1,0) and (2,2) as a vector."],"metadata":{"id":"RIcfDCTiA9xi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Boolean mask indexing: You can index with boolean tensors.\n","mask = x > 5\n","print(x[mask])  \n","\n","# In this example, x[mask] returns all elements in x that are greater than 5."],"metadata":{"id":"EjlqkZ36COM7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" Let's consider a 3D tensor for the following examples. We'll create a tensor with dimensions 3x3x3 for simplicity:"],"metadata":{"id":"Js0i-FkSChiS"}},{"cell_type":"code","source":["# Create a 3D tensor\n","x = torch.tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n","                  [[10, 11, 12], [13, 14, 15], [16, 17, 18]],\n","                  [[19, 20, 21], [22, 23, 24], [25, 26, 27]]])\n","x, x.shape"],"metadata":{"id":"26BTuDuGCj2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select the second column from every matrix\n","print(x[:, :, 1])  "],"metadata":{"id":"llOTIkuDCscC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select the second row from the every matrix\n","print(x[:, 1, :]) "],"metadata":{"id":"-WNx1NLyEE0N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select all rows and colums of the second matrix\n","print(x[1, :, :])"],"metadata":{"id":"ctsr4TIpEoIf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select the second row of the second matrix\n","print(x[1, 1, :])"],"metadata":{"id":"zAPJgI4kFSsr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select the second column of the second matrix\n","print(x[1, :, 1])"],"metadata":{"id":"khjNf-v7FdIM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select the first element in the second column of the second matrix\n","print(x[1, 0, 1])"],"metadata":{"id":"fY9jKptuFjnG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select the first element in the second row of the second matrix\n","print(x[1, 1, 0])"],"metadata":{"id":"2Xw5_AZHFzeV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The three parts of the index for a 3D tensor x[i, j, k] represent:\n","\n","i - Depth: This represents the index of the 2D matrix in the 3D tensor.\n","\n","j - Row: This represents the row index within the selected 2D matrix.\n","\n","k - Column: This represents the column index within the selected row of the 2D matrix.\n","\n","Also, note that Python uses 0-based indexing, so the first element is at index 0."],"metadata":{"id":"kWwVmImnGk_y"}},{"cell_type":"markdown","source":["### **More examples of slicing**"],"metadata":{"id":"H9HLeeVSGoB3"}},{"cell_type":"code","source":["import torch\n","\n","# Create a 3D tensor\n","x = torch.tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n","                  [[10, 11, 12], [13, 14, 15], [16, 17, 18]],\n","                  [[19, 20, 21], [22, 23, 24], [25, 26, 27]]])\n","\n","x, x.shape"],"metadata":{"id":"-WhOO0_gKAHk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It is important to note the elements included when slicing before and after a given element:"],"metadata":{"id":"7_Nm7DNMMxUz"}},{"cell_type":"code","source":["# Get the first 2 matrices, all rows, first 2 columns\n","print(x[:2, :, :2]) \n","\n","# Note: Slicing before an element such as \":2\" returns all elements up to index 2, but does not include index 2 (or 3rd element)"],"metadata":{"id":"Oe5fItvcHM_1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# From the second 2 matrices, all rows, first 2 columns\n","print(x[1:, :, :2]) \n","\n","# Note: Slicing after and element such as \"1:\" returns all elements after index 1 and includes index 1 (or 2nd element)"],"metadata":{"id":"IjsltTzSHhcg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print all rows and first two columns from the first and last matrix\n","result = x[[0, -1], :, :2]  # -1 is used to index the last element\n","print(result)"],"metadata":{"id":"NaAC8MlwIC9a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print first and last rows and all columns from the 2nd 2 matrices\n","result = x[1:, [0, -1], :]  # -1 is used to index the last element\n","print(result)"],"metadata":{"id":"j1c-gmzaJd8B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print first and last colums and all rows from the 1st 2 matrices\n","result = x[:2, :, [0, -1]]  # -1 is used to index the last element\n","print(result)"],"metadata":{"id":"KFXAni48KTwm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **PyTorch and NumPy**\n","\n","In PyTorch, NumPy is often used alongside as a complementary library. PyTorch provides a multi-dimensional array called a **Tensor** as its primary data structure for representing and manipulating data, similar to NumPy's **ndarray**. Both Tensors and ndarrays provide efficient storage and operations on multi-dimensional arrays of numerical data.\n","\n","The PyTorch library is built to work seamlessly with NumPy arrays. You can convert a NumPy array to a PyTorch Tensor and vice versa. PyTorch provides a function called `torch.from_numpy(ndarray)` to convert a NumPy array to a PyTorch Tensor and `torch.Tensor.numpy()` to convert a PyTorch Tensor to a NumPy array.\n","\n","***This integration allows you to take advantage of NumPy's extensive ecosystem of functions for numerical computation, data manipulation, and visualization alongside PyTorch's powerful deep learning capabilities.*** You can use NumPy operations on PyTorch Tensors and vice versa, making it convenient for data preprocessing, data loading, and post-processing tasks in machine learning pipelines.\n","\n","Overall, NumPy in PyTorch acts as a bridge between the deep learning framework and the broader scientific computing ecosystem, enabling you to leverage the strengths of both libraries in your machine learning workflows.\n","\n","**Documentation**\n","\n","Pytorch and Numpy: https://pytorch.org/tutorials/beginner/examples_tensor/polynomial_numpy.html"],"metadata":{"id":"jWedRDL8NsZQ"}},{"cell_type":"code","source":["# NumPy array to tensor\n","import torch\n","import numpy as np\n","\n","array = np.arange(1.0, 8.0)\n","tensor = torch.from_numpy(array) \n","array, tensor\n","\n","# note when converting from numpy -> pytorch, pytorch reflects numpy's default dtype (float64)"],"metadata":{"id":"G2qtzPldPTBw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["array.dtype"],"metadata":{"id":"uFnQabr1Rp6G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tensor.dtype"],"metadata":{"id":"_lLMrcxtRsRu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Defalut torch dtype\n","torch.arange(1.0, 8.0).dtype"],"metadata":{"id":"kqp70jhfRuNJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Note: changing the value of original array does not effect the tensor once generated\n","array = array + 1\n","array, tensor"],"metadata":{"id":"NYJ5MrBfRzZ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tensor to Numpy array\n","tensor = torch.ones(7)\n","numpy_tensor  = tensor.numpy()\n","tensor, numpy_tensor\n","\n","# note when converting from pytorch -> numpy, numpy reflects pytorch's default dtype (float32)"],"metadata":{"id":"QL2fruAGSmLT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Note: changing the value of the original tensor does not effect the array once generated.\n","tensor = tensor + 1\n","tensor, numpy_tensor"],"metadata":{"id":"cy2ZFjrJS7Xi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Converting 3D NumPy array to PyTorch Tensor\n","numpy_array = np.array([[[1, 2, 3], [4, 5, 6]],\n","                       [[7, 8, 9], [10, 11, 12]],\n","                       [[13, 14, 15], [16, 17, 18]]])\n","torch_tensor = torch.from_numpy(numpy_array)\n","numpy_array, torch_tensor"],"metadata":{"id":"vARAony7T-7B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numpy_array.dtype, torch_tensor.dtype"],"metadata":{"id":"EWNDadXJWeQX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Converting PyTorch Tensor to 3D NumPy array\n","torch_tensor2 = torch.tensor([[[19, 20, 21], [22, 23, 24]],\n","                             [[25, 26, 27], [28, 29, 30]],\n","                             [[31, 32, 33], [34, 35, 36]]])\n","numpy_array2 = torch_tensor2.numpy()\n","torch_tensor2, numpy_array2"],"metadata":{"id":"-uywx3Y1WJWi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch_tensor2.dtype, numpy_array2.dtype"],"metadata":{"id":"eNKOS-90WmT3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dO9uM4l2WqZs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Reproducibility and Random Seed**\n","\n","Reproducibility is crucial in machine learning and deep learning because it allows researchers and practitioners to verify and compare results, debug code, and ensure that experiments are consistent and reliable.\n","\n","However, achieving reproducibility in PyTorch can be challenging due to various factors, including parallel computations, GPU utilization, and random number generation. \n","\n",">**Definition**: Reproducibility refers to the ability to obtain consistent and deterministic results when running your code. It means that given the same input and random seed, your PyTorch code should produce identical results, including the same model weights, the same intermediate outputs, and the same random number sequences.\n","\n","[Reproducibility Documentation:](https://pytorch.org/docs/stable/notes/randomness.html)<br>\n","[Random Seed Wiki:](https://en.wikipedia.org/wiki/Random_seed)\n","\n","How neural networks learn: \n","1. Start with random numbers\n","2. Tensor operations \n","3. Update random numbers try to make them better representations of the data\n","4. Repeat again and again and again\n","\n","In order to reduce randomness in neural networks and PyTorch comes the concept of **Random Seed**: \n","\n",">**Definition**: Set a random seed at the beginning of your code using `torch.manual_seed` to ensure that random number generation is consistent across different runs. Additionally, set seeds for other libraries or modules that involve randomness, such as numpy or CUDA.\n","\n"],"metadata":{"id":"kLBOFcOwcxTw"}},{"cell_type":"code","source":["import torch\n","\n","# Create 2 random tensors\n","random_tensor_A = torch.rand(3, 4)\n","random_tensor_B = torch.rand(3, 4)\n","\n","print(random_tensor_A)\n","print(random_tensor_B)\n","print(random_tensor_A == random_tensor_B) # checks if any elements are equal "],"metadata":{"id":"pD9YswLHc0ZE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create random, but reproducibple tensors\n","\n","# Set the random seed\n","RANDOM_SEED = 42 # Because 42 is the answer to the universe! \n","torch.manual_seed(RANDOM_SEED)\n","random_tensor_C = torch.rand(3, 4)\n","\n","torch.manual_seed(RANDOM_SEED)\n","random_tensor_D = torch.rand(3, 4)\n","\n","print(random_tensor_C)\n","print(random_tensor_D)\n","print(random_tensor_C == random_tensor_D)"],"metadata":{"id":"uq-ZQbJ4gJgD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Accessing a GPU**\n","\n","\n","Accessing a GPU when using PyTorch can offer significant benefits, especially for deep learning tasks. Here are a few reasons why you would want to utilize a GPU with PyTorch:\n","\n","1. Accelerated Computation\n","2. Model Training Efficiency\n","3. Larger Model Capacity\n","4. Real-time Inference\n","5. Libraries and Frameworks\n","\n","To access a GPU with PyTorch, you need to ensure that you have the necessary hardware (an NVIDIA GPU) and compatible drivers installed. PyTorch provides CUDA support, allowing you to move tensors and models to the GPU and perform computations on it by using the .to() method or explicitly calling .cuda() or .cuda(device) on tensors and models.\n","\n"],"metadata":{"id":"OFWtPum2m4cI"}},{"cell_type":"markdown","source":["### **GPU Options:** <br>\n","1. Easiest - Google Colab free, or Google Colab Pro ($10 a month)\n","2. Use your own - Need to find a good Nvidia card, or figure out options for my AMD card (a little trickier, still in beta) \n","3. Cloud computing - GCP, AWS, Azure rent them. \n","\n","For 2 and 3 drivers (CUDA) need to be set up: https://pytorch.org/get-started/locally/i <br>\n","Additional resources for later: <br>\n","https://pytorch.org/docs/stable/notes/cuda.html <br>\n","https://pytorch.org/docs/stable/cuda.html <br>\n","https://github.com/IgorSusmelj/pytorch-styleguide"],"metadata":{"id":"zMII9TDDfIH5"}},{"cell_type":"code","source":["# For Google Colab GPU, change runtime type, select GPU, save, and run this line to check\n","!nvidia-smi"],"metadata":{"id":"m10QcpNRia-6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Check for GPU access with PyTorch:** <br>\n"],"metadata":{"id":"oSZffWyljX4g"}},{"cell_type":"code","source":["# Once GPU is selected and checked from previous seciont run this\n","import torch\n","torch.cuda.is_available()\n","\n","# returns True if functioning properly"],"metadata":{"id":"uDDW4imqkxtu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since PyTorch is capable of running compute on GPU and CPU, it's best practice to setup device agnostic code: <br>\n","\n","E.g. rund on GPU if avialable, else defalt to CPU"],"metadata":{"id":"LQKe1q4sl6nG"}},{"cell_type":"code","source":["# Setup device agnostic code\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"metadata":{"id":"VhHtzW9alEym"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count number of devices\n","torch.cuda.device_count()\n","\n","# For way later if running on multiple GPU's "],"metadata":{"id":"5FTm3wTdlZ5U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Moving tensors (and models) between GPU and CPU**\n","\n","By using these methods, you can easily move tensors between the CPU and GPU based on the availability of GPU resources and take advantage of GPU acceleration for computations.\n","\n"],"metadata":{"id":"f_Ytf7Kflfxo"}},{"cell_type":"code","source":["import torch\n","\n","# Check if GPU is available\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")  # Use GPU\n","else:\n","    device = torch.device(\"cpu\")  # Use CPU\n","\n","# Create a tensor (default on the CPU)\n","tensor = torch.tensor([1, 2, 3], )\n","\n","# Tensor not on GPU\n","print(tensor, tensor.device)\n"],"metadata":{"id":"2c2yFMMNohOb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Move tensor to GPU (if available)\n","tensor_on_gpu = tensor.to(device)\n","tensor_on_gpu"],"metadata":{"id":"AdgEbO8gpeWh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# If tensor is on GPU, can't transform it to Numpy\n","# tensor_on_gpu.numpy() returns following error\n","# TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n","# To fix GPU tensor issue with NumPy, first set it back to the CPU\n","tensor_on_cpu = tensor_on_gpu.cpu().numpy()\n","tensor_on_cpu"],"metadata":{"id":"9AxNUXLKpvIi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**End of Fundamentals.**"],"metadata":{"id":"QQyud51Sr_7-"}}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["Sz8GEtV5aTB8","xYyR3avBXa3E","OLJhCuD9c9yL","jkxKKJfheNrx","JlTRjzODgnoO","IUh8kzYJnVuU","3V0fS4v1pRaW","53XS8246sIRK","AyAWsuBRF3hn"],"gpuType":"T4","authorship_tag":"ABX9TyOCM7U73p7rKidMUMI3TYSO"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}